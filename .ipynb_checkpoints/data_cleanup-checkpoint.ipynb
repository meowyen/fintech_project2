{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Reuters corpus\n",
    "# import nltk\n",
    "# nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from newsapi import NewsApiClient\n",
    "from nltk.corpus import reuters\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize News API\n",
    "news_api_key = os.getenv(\"NEWS_API_KEY\")\n",
    "newsapi = NewsApiClient(api_key=news_api_key)\n",
    "\n",
    "def get_newsapi_articles(q):\n",
    "    \"\"\"\n",
    "    Return the list of articles for specified query terms.\n",
    "    \"\"\"\n",
    "\n",
    "    data = newsapi.get_everything(q=q, language=\"en\", page_size=100)\n",
    "    articles = data[\"articles\"]\n",
    "    print(f\"Found {len(articles)} articles for '{q}''.\")\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bing News API\n",
    "bing_subscription_key = os.getenv(\"BING_API_KEY\")\n",
    "headers = {\"Ocp-Apim-Subscription-Key\" : bing_subscription_key}\n",
    "bing_search_url = \"https://api.bing.microsoft.com/v7.0/news/search\"\n",
    "\n",
    "def get_bing_articles(q):\n",
    "    \"\"\"\n",
    "    Return the list of articles for specified query terms.\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\"q\": q, \"textDecorations\": True, \"textFormat\": \"HTML\", \"count\": 100}\n",
    "\n",
    "    response = requests.get(bing_search_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    articles = response.json()[\"value\"]\n",
    "\n",
    "    print(f\"{len(articles)} articles found\")\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardian News API\n",
    "guardian_api_key = os.getenv(\"GUARDIAN_API_KEY\")\n",
    "guardian_search_url = \"https://content.guardianapis.com/search\"\n",
    "guardian_tags_url = \"http://content.guardianapis.com/tags\"\n",
    "\n",
    "relevant_tags = {\n",
    "    \"weapons_defense\": [\"science/weaponstechnology\", \"world/chemical-weapons\", \"us-news/us-military\", \"world/drones\"],\n",
    "    \"human_rights\": [\"global-development/human-rights\", \"society/equality-and-human-rights-commission-ehrc\", \"sustainable-business/business-human-rights\"],\n",
    "    \"animal_rights\": [\"science/animal-experimentation\", \"world/animal-welfare\"],\n",
    "    \"environment\": [\"environment/climate-change\", \"carbonreduction/carbonreduction\", \"environment/carbon-emissions\", \"environment/carbon-offset-projects\", \"environment/carbonfootprints\", \"environment/renewableenergy\", \"environment/sustainable-development\", \"global-development/sustainable-development-goals\", \"social-progress-imperative/social-progress-imperative\"]\n",
    "}\n",
    "\n",
    "def get_guardian_articles(q, section_id = None, tag_id = None):\n",
    "    \"\"\"\n",
    "    Return the list of articles for specified query terms and optional tag ID.\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\"q\": q, \"from-date\": \"2016-01-01\", \"show-tags\": \"all\", \"page-size\": 200, \"api-key\": guardian_api_key, \"show-fields\": \"body\", \"tag\": tag_id, \"sectionId\": section_id}\n",
    "\n",
    "    response = requests.get(guardian_search_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    response_json = response.json()[\"response\"]\n",
    "    results = response_json[\"results\"]\n",
    "\n",
    "    print(f\"{len(results)} articles found\")\n",
    "\n",
    "    for article in results:\n",
    "        html_text = article[\"fields\"][\"body\"]\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        article[\"text\"] = soup.get_text()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_guardian_articles_by_tag(tag_id):\n",
    "    \"\"\"\n",
    "    Return the list of articles for specified tag ID.\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\"show-tags\": \"all\", \"from-date\": \"2016-01-01\", \"page-size\": 200, \"api-key\": guardian_api_key, \"tag\": tag_id, \"show-fields\": \"body\"}\n",
    "\n",
    "    response = requests.get(guardian_search_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    response_json = response.json()[\"response\"]\n",
    "    results = response_json[\"results\"]\n",
    "\n",
    "    print(f\"{len(results)} articles found\")\n",
    "\n",
    "    for article in results:\n",
    "        html_text = article[\"fields\"][\"body\"]\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        article[\"text\"] = soup.get_text()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_paged_guardian_tags(page, tag_type = \"keyword\", page_size = 1000):\n",
    "    \"\"\"\n",
    "    Return the list of tags for specified page.\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\"page-size\": page_size, \"page\": page, \"type\": tag_type, \"api-key\": guardian_api_key}\n",
    "\n",
    "    response = requests.get(guardian_tags_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    response_json = response.json()[\"response\"]\n",
    "\n",
    "    return response_json\n",
    "\n",
    "\n",
    "def get_guardian_tags(tag_type = \"keyword\"):\n",
    "    \"\"\"\n",
    "    Return the list of tags.\n",
    "    \"\"\"\n",
    "\n",
    "    page = 1\n",
    "    pages = None\n",
    "    page_size = 1000\n",
    "    tags = []\n",
    "\n",
    "    response = get_paged_guardian_tags(page, page_size=1)\n",
    "    pages = response[\"total\"] / page_size\n",
    "\n",
    "    while page < pages:\n",
    "        print(f\"Retrieving tags of page {page}\")\n",
    "        response = get_paged_guardian_tags(page, tag_type=tag_type, page_size=page_size)\n",
    "        tags = tags + response[\"results\"]\n",
    "        page = page + 1\n",
    "\n",
    "    return tags\n",
    "\n",
    "\n",
    "def export_guardian_tags():\n",
    "    \"\"\"\n",
    "    Export Guardian keyword tags to CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    tags = get_guardian_tags()\n",
    "    tags_df = pd.DataFrame(tags)\n",
    "    tags_filepath = Path(\"data/guardian_tags.csv\")\n",
    "    tags_df[[\"id\", \"sectionId\", \"sectionName\", \"webTitle\", \"apiUrl\"]].to_csv(tags_filepath, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get articles for query terms and section ID\n",
    "articles = get_guardian_articles(\"Tesla\", section_id=\"environment\")\n",
    "articles_df = pd.DataFrame(articles)\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get articles for query terms and tag ID\n",
    "articles = get_guardian_articles(\"Tesla\", tag_id=\"environment/climate-change\")\n",
    "articles_df = pd.DataFrame(articles)\n",
    "articles_df.head()"
   ]
  }
 ]
}